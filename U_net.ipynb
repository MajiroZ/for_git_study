{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQNaYKUJ+Q41g/2a7D7TDL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MajiroZ/for_git_study/blob/master/U_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##【問題1】学習・推定"
      ],
      "metadata": {
        "id": "fAJNeZDH_N6t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "GK6hsDUczgHs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f13f9e74-3f66-4f0a-ed29-b119a97b3df8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-01-08 12:19:26.843747: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-08 12:19:26.870412: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-08 12:19:26.878021: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-08 12:19:26.895161: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-08 12:19:28.344715: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Found 30 images belonging to 1 classes.\n",
            "Found 30 images belonging to 1 classes.\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - accuracy: 0.4847 - loss: 0.6805\n",
            "Epoch 1: loss improved from inf to 0.60465, saving model to unet_membrane.keras\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 270ms/step - accuracy: 0.4853 - loss: 0.6803\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/main.py\", line 21, in <module>\n",
            "    results = model.predict(testGene, steps=30, verbose=1)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/generator_data_adapter.py\", line 17, in __init__\n",
            "    raise ValueError(\n",
            "ValueError: When passing a Python generator to a Keras model, the generator must return a tuple, either (input,) or (inputs, targets) or (inputs, targets, sample_weights). Received: [[[[0.71135633]\n",
            "   [0.7496376 ]\n",
            "   [0.79347998]\n",
            "   ...\n",
            "   [0.48786609]\n",
            "   [0.48072224]\n",
            "   [0.57921191]]\n",
            "\n",
            "  [[0.74566155]\n",
            "   [0.78702021]\n",
            "   [0.78341304]\n",
            "   ...\n",
            "   [0.49891909]\n",
            "   [0.54408408]\n",
            "   [0.45938778]]\n",
            "\n",
            "  [[0.80223938]\n",
            "   [0.78691477]\n",
            "   [0.810324  ]\n",
            "   ...\n",
            "   [0.56314613]\n",
            "   [0.52451802]\n",
            "   [0.34467166]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.64970307]\n",
            "   [0.58171255]\n",
            "   [0.56613815]\n",
            "   ...\n",
            "   [0.77317005]\n",
            "   [0.68313019]\n",
            "   [0.77834483]]\n",
            "\n",
            "  [[0.53374815]\n",
            "   [0.46384348]\n",
            "   [0.51799164]\n",
            "   ...\n",
            "   [0.66697115]\n",
            "   [0.5689424 ]\n",
            "   [0.79931432]]\n",
            "\n",
            "  [[0.37782069]\n",
            "   [0.52483454]\n",
            "   [0.503447  ]\n",
            "   ...\n",
            "   [0.54928984]\n",
            "   [0.63644053]\n",
            "   [0.74103752]]]]\n"
          ]
        }
      ],
      "source": [
        "!python main.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##【問題2】コードリーディング"
      ],
      "metadata": {
        "id": "OxWSm6lC3xOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### U-Netコードリーディングの要点まとめ\n",
        "\n",
        "#### 1. 中核部分\n",
        "`model.py`でのU-Netモデルの重要部分\n",
        "\n",
        "```python\n",
        "up6 = Conv2D(512, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2,2))(drop5))\n",
        "merge6 = concatenate([drop4, up6], axis=3)\n",
        "```\n",
        "\n",
        "この部分では、`UpSampling2D`を使用して特徴マップの空間解像度を2倍にし、`concatenate`関数で下位層の特徴マップと結合する。これにより、U-Netのスキップ接続が実現される。これがU-Netの特徴であり、高精度なセグメンテーションを可能にしている。\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. 使用ライブラリのインポート\n",
        "```python\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import skimage.transform as trans\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras import backend as keras\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (12, 9)\n",
        "```\n",
        "Kerasを中心に、画像処理のために`cv2`や`skimage`を利用。`ModelCheckpoint`で学習中のモデルの保存も行う\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. データの読み込み\n",
        "```python\n",
        "image_path_list = glob.glob(\"competition_data/train/images/*\")\n",
        "X_train, y_train = [], []\n",
        "\n",
        "for image_path in image_path_list[:100]:\n",
        "    image = cv2.resize(cv2.imread(image_path, cv2.IMREAD_GRAYSCALE) / 255, (256, 256))\n",
        "    X_train.append(image)\n",
        "    mask = cv2.resize(cv2.imread(image_path.replace(\"images\", \"masks\"), cv2.IMREAD_GRAYSCALE) / 255, (256, 256))\n",
        "    y_train.append(mask)\n",
        "\n",
        "X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "y_train = np.expand_dims(y_train, axis=-1)\n",
        "```\n",
        "1. 画像とマスクのパスをリスト化\n",
        "2. グレースケール画像を正規化しつつリサイズ\n",
        "3. `X_train`と`y_train`にそれぞれ画像とマスクを追加\n",
        "4. `numpy.array`に変換して学習用の形に整形\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. U-Netモデルの定義\n",
        "```python\n",
        "def unet(pretrained_weights=None, input_size=(256, 256, 1)):\n",
        "    inputs = Input(input_size)\n",
        "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
        "    conv1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    \n",
        "    # 略 (他の層も同様に定義)\n",
        "\n",
        "    conv10 = Conv2D(1, 1, activation='sigmoid', name=\"output\")(conv9)\n",
        "    model = Model(inputs=inputs, outputs=conv10)\n",
        "\n",
        "    model.compile(optimizer=Adam(lr=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    if pretrained_weights:\n",
        "        model.load_weights(pretrained_weights)\n",
        "\n",
        "    return model\n",
        "```\n",
        "- U-Netモデルはエンコーダとデコーダの2つの部分で構成。\n",
        "- スキップ接続により、エンコーダの特徴マップをデコーダ側で使用。\n",
        "- 出力層はシグモイド活性化関数で、二値セグメンテーションに対応。\n",
        "\n",
        "---\n",
        "\n",
        "#### 5. モデルの学習と保存\n",
        "```python\n",
        "model_checkpoint = ModelCheckpoint('unet_membrane.hdf5', monitor='loss', verbose=1, save_best_only=True)\n",
        "model = unet()\n",
        "model.fit(X_train, y_train, batch_size=3, epochs=1, callbacks=[model_checkpoint])\n",
        "```\n",
        "学習途中で最良のモデルを`unet_membrane.hdf5`に保存\n",
        "\n",
        "---\n",
        "\n",
        "#### 6. 推論\n",
        "```python\n",
        "y_pred = model.predict(np.array([X_train[30, :, :, 0]]))\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(y_pred[0, :, :, 0], cmap='gray')\n",
        "ax[1].imshow(y_train[30, :, :, 0], cmap='gray')\n",
        "```\n",
        "- `model.predict`で画像を入力し、出力マスクを取得。\n",
        "- Matplotlibで予測結果と正解ラベルを並べて可視化。\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-nqYHokA7f4h"
      }
    }
  ]
}