{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MajiroZ/for_git_study/blob/master/Deep_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6Kzv_FxHkLg"
      },
      "source": [
        "##【問題1】全結合層のクラス化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aUif3Ja4LzIg"
      },
      "outputs": [],
      "source": [
        "# 全結合層をクラス化する\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class FC:\n",
        "\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "        self.X = None\n",
        "        self.dW = None\n",
        "        self.dB = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        out = np.dot(X, self.W) + self.B\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.X.T, dout)\n",
        "        self.dB = np.sum(dout, axis=0)\n",
        "        self = self.optimizer.update(self)\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpONLCbQKNSR"
      },
      "source": [
        "##【問題2】初期化方法のクラス化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W27Ohv-sMJbL"
      },
      "outputs": [],
      "source": [
        "# 全結合層のコンストラクタに初期化方法のインスタンスを渡せるようにする\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    ガウス分布によるシンプルな初期化\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      ガウス分布の標準偏差\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "    def B(self, n_nodes2):\n",
        "        B = self.sigma * np.random.randn(n_nodes2)\n",
        "        return B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpGgOEZZKgY1"
      },
      "source": [
        "##【問題3】最適化手法のクラス化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "65SE3BRFMmpH"
      },
      "outputs": [],
      "source": [
        "class SGD:\n",
        "    \"\"\"\n",
        "    確率的勾配降下法\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : 学習率\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "    def update(self, layer):\n",
        "        layer.W -= self.lr * layer.dW\n",
        "        layer.B -= self.lr * layer.dB\n",
        "        return layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP2CL4XmKy_B"
      },
      "source": [
        "##【問題4】活性化関数のクラス化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "293ynYgbM2vH"
      },
      "outputs": [],
      "source": [
        "class Tanh:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        out = np.tanh(X)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1 - self.out**2)\n",
        "        return dx\n",
        "\n",
        "class Softmax:\n",
        "    def __init__(self):\n",
        "        self.loss = None # 損失\n",
        "        self.y = None    # softmaxの出力\n",
        "        self.t = None    # 教師データ\n",
        "\n",
        "    def forward(self, x, t=None):\n",
        "        if t is not None:\n",
        "            self.t = t\n",
        "            self.y = softmax(x)\n",
        "            self.loss = cross_entropy_error(self.y, self.t)\n",
        "            return self.loss\n",
        "        else:\n",
        "            self.y = softmax(x)\n",
        "            return self.y\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        dx = (self.y - self.t) / batch_size # Calculate the gradient of the loss with respect to the input\n",
        "        return dx # Return the gradient\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T\n",
        "\n",
        "    x = x - np.max(x) # オーバーフロー対策\n",
        "    return np.exp(x) / np.sum(np.exp(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQfh0bKZNcHI"
      },
      "source": [
        "##【問題5】ReLUクラスの作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "d9h6fRj6NbzA"
      },
      "outputs": [],
      "source": [
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnXysuo6NzNk"
      },
      "source": [
        "##【問題6】重みの初期値"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9kEQRX_yNxuD"
      },
      "outputs": [],
      "source": [
        "class XavierInitializer:\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2) / np.sqrt(n_nodes1) # Xavier Initialization\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2): # Added the B method for bias initialization\n",
        "        B = self.sigma * np.random.randn(n_nodes2) / np.sqrt(n_nodes2) # Xavier Initialization for biases\n",
        "        return B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wPASFO0cN5wv"
      },
      "outputs": [],
      "source": [
        "class HeInitializer:\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2) * np.sqrt(2 / n_nodes1) # He Initialization\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):  # Added the B method for bias initialization\n",
        "        B = self.sigma * np.random.randn(n_nodes2) * np.sqrt(2 / n_nodes2) # He Initialization for biases\n",
        "        return B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-sG2TKnN_Qn"
      },
      "source": [
        "##【問題7】最適化手法"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KA2W4NvXOZSo"
      },
      "outputs": [],
      "source": [
        "class AdaGrad:\n",
        "\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "        self.h = None  # Initialize h as a dictionary\n",
        "\n",
        "    def update(self, layer):\n",
        "        # If h is not initialized, create a dictionary to store h for each layer\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "\n",
        "        # Get the layer's unique identifier (e.g., its id)\n",
        "        layer_id = id(layer)\n",
        "\n",
        "        # Get h for the current layer, initialize if not present\n",
        "        if layer_id not in self.h:\n",
        "            self.h[layer_id] = {'W': np.zeros_like(layer.W), 'B': np.zeros_like(layer.B)}\n",
        "\n",
        "        # Update weights\n",
        "        self.h[layer_id]['W'] += layer.dW * layer.dW\n",
        "        layer.W -= self.lr * layer.dW / (np.sqrt(self.h[layer_id]['W']) + 1e-7)\n",
        "\n",
        "        # Update biases\n",
        "        self.h[layer_id]['B'] += layer.dB * layer.dB\n",
        "        layer.B -= self.lr * layer.dB / (np.sqrt(self.h[layer_id]['B']) + 1e-7)\n",
        "\n",
        "        return layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khxvqwC_OxPE"
      },
      "source": [
        "##【問題8】クラスの完成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RHpj1HVaPUtr"
      },
      "outputs": [],
      "source": [
        "# 任意の層数に拡張しやすいネットワークのクラスScratchDeepNeuralNetrowkClassifier\n",
        "\n",
        "class ScratchDeepNeuralNetrowkClassifier:\n",
        "\n",
        "    def __init__(self, n_nodes, n_output, activation, initializer, optimizer, n_input):\n",
        "        self.n_nodes = n_nodes\n",
        "        self.n_output = n_output\n",
        "        self.activation = activation\n",
        "        self.initializer = initializer\n",
        "        self.optimizer = optimizer\n",
        "        self.layers = None\n",
        "        self.loss = None\n",
        "        self.n_input = n_input\n",
        "\n",
        "    def _forward(self,X=None):\n",
        "        for layer in self.layers[:-1]:\n",
        "            X = layer.forward(X)\n",
        "\n",
        "        self.loss = self.layers[-1].forward(X, self.t)\n",
        "        return X\n",
        "\n",
        "    def fit(self, X, y, epoch=100):\n",
        "        self.layers = []\n",
        "        # First layer: connect input to the first hidden layer\n",
        "        layer = FC(self.n_input, self.n_nodes[0], self.initializer, self.optimizer) # Changed to use n_input\n",
        "        self.layers.append(layer)\n",
        "        if self.activation == \"relu\":\n",
        "            self.layers.append(Relu())\n",
        "        elif self.activation == \"tanh\":\n",
        "            self.layers.append(Tanh())\n",
        "        else:\n",
        "            pass\n",
        "        # Remaining layers\n",
        "        for i in range(len(self.n_nodes) - 1):\n",
        "            layer = FC(self.n_nodes[i], self.n_nodes[i+1], self.initializer, self.optimizer)\n",
        "            self.layers.append(layer)\n",
        "            if self.activation == \"relu\":\n",
        "                self.layers.append(Relu())\n",
        "            elif self.activation == \"tanh\":\n",
        "                self.layers.append(Tanh())\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "        self.layers.append(FC(self.n_nodes[-1], self.n_output, self.initializer, self.optimizer))\n",
        "        self.layers.append(Softmax())\n",
        "\n",
        "        for i in range(epoch):\n",
        "            for x, t in zip(X,y):\n",
        "                x = x.reshape(1,-1)\n",
        "                t = t.reshape(1,-1)\n",
        "                self.t = t\n",
        "                self._forward(x)\n",
        "                self._backward(t)\n",
        "\n",
        "\n",
        "    def _backward(self,t):\n",
        "        dout = 1\n",
        "        dout = self.layers[-1].backward(dout)\n",
        "        for layer in reversed(self.layers[:-1]):\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "    def predict(self,X):\n",
        "        X = X.reshape(X.shape[0], -1)\n",
        "        for layer in self.layers:\n",
        "            X = layer.forward(X)\n",
        "        return np.argmax(X,axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMLmEacmP33B"
      },
      "source": [
        "##【問題9】学習と推定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG3NhpE1QBFW",
        "outputId": "7afc9905-4973-4c99-c24a-9923710a58f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# --- Improved Hyperparameters ---\n",
        "n_nodes = [400, 200, 100]  # Increased nodes and added a layer\n",
        "n_output = 10\n",
        "\n",
        "# Experiment with different initializers and optimizers\n",
        "initializers = {'he': HeInitializer(1)}\n",
        "optimizers = {'adagrad': AdaGrad(0.01)}\n",
        "activations = [\"tanh\"]  # Changed to tanh\n",
        "\n",
        "n_input = X_train.shape[1] * X_train.shape[2]\n",
        "\n",
        "epochs = 10  # Increased epochs\n",
        "\n",
        "\n",
        "# --- Create and Train the Model ---\n",
        "dnn = ScratchDeepNeuralNetrowkClassifier(n_nodes=n_nodes, n_output=n_output,\n",
        "                                        activation=activations[0],\n",
        "                                        initializer=list(initializers.values())[0],\n",
        "                                        optimizer=list(optimizers.values())[0],\n",
        "                                        n_input=n_input)\n",
        "dnn.fit(X_train, y_train, epoch=epochs)\n",
        "\n",
        "# --- Evaluate the Model ---\n",
        "y_pred = dnn.predict(X_test)\n",
        "\n",
        "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dL881RWYQW4x",
        "outputId": "7d707964-42cb-450d-de80-413a0d2d8248"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracyがとても低かったため、ノードを増やして計算してみたものが以下であるが、2時間以上の計算時間がかかり、エラーが出てしまったため、上のプログラムを以って一旦提出する。"
      ],
      "metadata": {
        "id": "jgHKMw0Z4b5_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "T_WQMM5wQGtS",
        "outputId": "39565fce-87c3-4f17-a956-ce67a2c1cf20"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with activation: relu, initializer: simple, optimizer: sgd\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-7342f83939db>:48: RuntimeWarning: invalid value encountered in subtract\n",
            "  x = x - np.max(x, axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.098\n",
            "Training with activation: relu, initializer: simple, optimizer: adagrad\n",
            "Accuracy: 0.0982\n",
            "Training with activation: relu, initializer: xavier, optimizer: sgd\n",
            "Accuracy: 0.098\n",
            "Training with activation: relu, initializer: xavier, optimizer: adagrad\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "operands could not be broadcast together with shapes (784,400) (400,200) (784,400) ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-494b96f8b065>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                                                 \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                                 optimizer=optimizer, n_input=n_input)\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-da9f37db5bec>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, epoch)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-da9f37db5bec>\u001b[0m in \u001b[0;36m_backward\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-eea94fc33bcf>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dout)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-c564e3411df2>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdW\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdW\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (784,400) (400,200) (784,400) "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "n_nodes = [400, 200, 100, 10]\n",
        "n_output = 10\n",
        "\n",
        "initializers = {'simple': SimpleInitializer(0.01), 'xavier': XavierInitializer(1), 'he':HeInitializer(1)}\n",
        "optimizers = {'sgd': SGD(0.01), 'adagrad': AdaGrad(0.01)}\n",
        "\n",
        "# 活性化関数を変える\n",
        "activations = [\"relu\", \"tanh\"]\n",
        "\n",
        "n_input = X_train.shape[1] * X_train.shape[2]\n",
        "\n",
        "# いくつかのネットワーク\n",
        "for activation in activations:\n",
        "  for init_name, initializer in initializers.items():\n",
        "    for opt_name, optimizer in optimizers.items():\n",
        "        print(f\"Training with activation: {activation}, initializer: {init_name}, optimizer: {opt_name}\")\n",
        "\n",
        "        dnn = ScratchDeepNeuralNetrowkClassifier(n_nodes=n_nodes, n_output=n_output,\n",
        "                                                activation=activation, initializer=initializer,\n",
        "                                                optimizer=optimizer, n_input=n_input)\n",
        "        dnn.fit(X_train, y_train, epoch=10)\n",
        "\n",
        "        y_pred = dnn.predict(X_test)\n",
        "\n",
        "        accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
        "        print(f\"Accuracy: {accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcn30wpCs04Wmz2c2Q9xI6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}