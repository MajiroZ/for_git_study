{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MajiroZ/for_git_study/blob/master/Deep_Neural_Network_%E3%81%AE%E3%82%B3%E3%83%94%E3%83%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "B7DxJ9IIINI2"
      },
      "outputs": [],
      "source": [
        "# Import\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "z9v2VqKTIRFr"
      },
      "outputs": [],
      "source": [
        "# Evaluation index\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "5ykD6s-SJJcw"
      },
      "outputs": [],
      "source": [
        "# Evaluation index\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "7ITHZffXJNSc"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "from keras.datasets import mnist\n",
        "(X, y), (X_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "SzCuL6smJXwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e57d1c6-c7bb-4792-a300-5419e18f513d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(60000, 28, 28)\n",
            "uint8\n"
          ]
        }
      ],
      "source": [
        "# Check the data\n",
        "print(X.shape) # (60000, 28, 28)\n",
        "print(X.shape) # (10000, 28, 28)\n",
        "print(X[0].dtype) # uint8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "FPpRJddEJeOZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2af013d2-2698-46af-f824-8566a8991dca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ]
        }
      ],
      "source": [
        "# Smoothing\n",
        "X_flat = X.reshape(-1, 784)\n",
        "X_test_flat = X_test.reshape(-1, 784)\n",
        "print(X_flat.shape)\n",
        "print(X_test_flat.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "yhmwii7FJgAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72509bc9-f4ff-461a-edd3-a3745ed8d421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "0.0\n"
          ]
        }
      ],
      "source": [
        "# Type conversion, normalization\n",
        "X_flat = X_flat.astype(np.float64)\n",
        "X_test_flat = X_test_flat.astype(np.float64)\n",
        "X_flat /= 255\n",
        "X_test_flat /= 255\n",
        "print(X_flat.max()) # 1.0\n",
        "print(X_flat.min()) # 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "SGnS9ljtJmBR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "408e05e7-ca5d-4a7c-d97a-82dd93fcc3cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000,)\n",
            "(60000, 10)\n",
            "float64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Remove the 'sparse' argument\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "y_one_hot = enc.fit_transform(y[:, np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
        "print(y.shape) # (60000,)\n",
        "print(y_one_hot.shape) # (60000, 10)\n",
        "print(y_one_hot.dtype) # float64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCY0ikwhJ5-G",
        "outputId": "ed6186ba-b196-4d2b-8211-1d9347fb144e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(48000, 784)\n",
            "(12000, 784)\n",
            "(48000, 10)\n",
            "(12000, 10)\n"
          ]
        }
      ],
      "source": [
        "# Split into training data and validation data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_flat, y_one_hot, test_size=0.2)\n",
        "print(X_train.shape)\n",
        "print(X_valid.shape)\n",
        "print(y_train.shape)\n",
        "print(y_valid.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "v4CyipTdJ_WZ"
      },
      "outputs": [],
      "source": [
        "# Scratch Deep Neural Network (for now, 3 layer NN)\n",
        "class ScratchDeepNeuralNetworkClassifier():\n",
        "    \"\"\"\n",
        "    N Layer Neural Network Classifier\n",
        "    Parameters\n",
        "    ----------\n",
        "    self.sigma : Standard deviation of Gaussian distribution\n",
        "    self.lr : learning rate\n",
        "    self.n_nodes1 : Number of nodes in the first layer\n",
        "    self.n_nodes2 : Number of nodes in the second layer\n",
        "    self.n_output : Number of nodes in the output layer\n",
        "\n",
        "    self.n_epoch : epoch number\n",
        "    self.n_batch : Number of batches\n",
        "    self.verbose : Visualizing the learning process\n",
        "    Attributes\n",
        "    ----------\n",
        "    \"\"\"\n",
        "    def __init__(self, n_features, n_nodes1, n_nodes2, n_output,\n",
        "                 sigma, n_epoch, n_batch, lr, verbose = False):\n",
        "        # Parameters\n",
        "        self.n_features = n_features\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        self.n_output = n_output\n",
        "        self.sigma = sigma\n",
        "        self.n_epoch = n_epoch\n",
        "        self.n_batch = n_batch\n",
        "        self.lr = lr\n",
        "        self.verbose = verbose\n",
        "        self.log_loss = np.zeros(self.n_epoch)\n",
        "\n",
        "    def loss_function(self,y,yt):\n",
        "        delta = 1e-7\n",
        "        return -np.mean(yt*np.log(y+delta))\n",
        "\n",
        "    def fit(self, X, y, X_val=False, y_val=False):\n",
        "        \"\"\"\n",
        "        Train a neural network classifier.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray of the following form, shape (n_samples, n_features)\n",
        "            Features of training data\n",
        "        y : ndarray of the following form, shape (n_samples, )\n",
        "            Correct answer value of training data\n",
        "        X_val : ndarray of the following form, shape (n_samples, n_features)\n",
        "            Features of validation data\n",
        "        y_val : ndarray of the following form, shape (n_samples, )\n",
        "            Correct value of validation data\n",
        "        \"\"\"\n",
        "\n",
        "        optimizer1 = AdaGrad(self.lr)\n",
        "        optimizer2 = AdaGrad(self.lr)\n",
        "        optimizer3 = AdaGrad(self.lr)\n",
        "\n",
        "        initializer1 = XavierInitializer()\n",
        "        initializer2 = XavierInitializer()\n",
        "        initializer3 = SimpleInitializer(self.sigma)\n",
        "\n",
        "        self.FC1 = FC(self.n_features, self.n_nodes1, initializer1, optimizer1, Tanh())\n",
        "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, initializer2, optimizer2, Tanh())\n",
        "        self.FC3 = FC(self.n_nodes2, self.n_output, initializer3, optimizer3, Softmax())\n",
        "\n",
        "        for epoch in range(self.n_epoch):\n",
        "            # Mini-batch processing\n",
        "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.n_batch)\n",
        "\n",
        "            self.loss = 0\n",
        "            for mini_X_train, mini_y_train in get_mini_batch:\n",
        "\n",
        "                # Forward propagation\n",
        "                self.Z1 = self.FC1.forward(mini_X_train)\n",
        "                self.Z2 = self.FC2.forward(self.Z1)\n",
        "                self.Z3 = self.FC3.forward(self.Z2)\n",
        "\n",
        "                # Back propagation\n",
        "                self.dA3 = (self.Z3 - mini_y_train)/self.n_batch\n",
        "                self.dZ2 = self.FC3.backward(self.dA3)\n",
        "                self.dZ1 = self.FC2.backward(self.dZ2)\n",
        "                self.dZ0 = self.FC1.backward(self.dZ1)\n",
        "\n",
        "                # Loss function\n",
        "                self.loss += self.loss_function(self.Z3,mini_y_train)\n",
        "\n",
        "            self.log_loss[epoch] = self.loss/len(get_mini_batch)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Estimate using a neural network classifier.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray of the following form, shape (n_samples, n_features)\n",
        "            Sample\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            ndarray of the following form, shape (n_samples, 1)\n",
        "            Estimation results\n",
        "        \"\"\"\n",
        "        pred_Z1 = self.FC1.forward(X)\n",
        "        pred_Z2 = self.FC2.forward(pred_Z1)\n",
        "        return np.argmax(self.FC3.forward(pred_Z2),axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "zuKVBfC1KDSa"
      },
      "outputs": [],
      "source": [
        "# Mini-batch processing class\n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    Iterator to get the mini-batch\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : ndarray of the following form, shape (n_samples, n_features)\n",
        "      Training data\n",
        "    y : ndarray of the follwowing form, shape (n_samples, 1)\n",
        "      correct value\n",
        "    batch_size : int\n",
        "      Batch size\n",
        "    seed : int\n",
        "      Seeding random numbers in NumPy\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 20, seed=None):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(int)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1]\n",
        "\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qKuK14nKOXG"
      },
      "source": [
        "### 【Problem 1】Coupling layers\n",
        "Please classify all binding layers.\n",
        "  \n",
        "The following is a model. Initialize the weights and biases in the constructor, and then prepare the forward and backward methods. By keeping the weights W, bias B, and the input X for forwarding as instance variables, there is no need for complicated input and output.\n",
        "  \n",
        "Note that instances can also be passed as arguments. Therefore, if an instance initializer of the initialization method is received in the constructor, it will be initialized. The initialization method can be changed by changing the instance to be passed.  \n",
        "  \n",
        "You can also pass your own instance self as an argument. You can also pass your own instance self as an argument, and use this to update the layer weights as in self.optimizer.update(self). There are multiple values needed for the update, but they can all be instance variables that all coupled layers have.\n",
        "\n",
        "The initialization method and the class of optimization methods are described below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "LdOC7gI1KHga"
      },
      "outputs": [],
      "source": [
        "class FC:\n",
        "    \"\"\"\n",
        "    All coupling layers from number of nodes n_nodes1 to n_nodes2\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_nodes1 : int\n",
        "      Number of nodes in the previous layer\n",
        "    n_nodes2 : int\n",
        "      Number of nodes in subsequent layers\n",
        "    initializer : Instances of initialization methods\n",
        "    optimizer : Instances of optimization methods\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, activation):\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        self.initializer = initializer\n",
        "        self.optimizer = optimizer\n",
        "        self.activation = activation\n",
        "        # Initialize\n",
        "        self.W = self.initializer.W(self.n_nodes1, self.n_nodes2)\n",
        "        self.B = self.initializer.B(self.n_nodes2)\n",
        "\n",
        "        print(f\"FC Layer: Weights shape - {self.W.shape}, Bias shape - {self.B.shape}\")\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        forward\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray of the following form, shape (batch_size, n_nodes1)\n",
        "            Input\n",
        "        Returns\n",
        "        ----------\n",
        "        A : ndarray of the following form, shape (batch_size, n_nodes2)\n",
        "            Output\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        self.A = np.dot(self.X,self.W) + self.B\n",
        "\n",
        "        return self.activation.forward(self.A)\n",
        "\n",
        "    def backward(self, dZ):\n",
        "        \"\"\"\n",
        "        backward\n",
        "        Parameters\n",
        "        ----------\n",
        "        dA : ndarray of the following form, shape (batch_size, n_nodes2)\n",
        "            The gradient flowed in from behind.\n",
        "        Returns\n",
        "        ----------\n",
        "        dZ : ndarray of the following form, shape (batch_size, n_nodes1)\n",
        "            forward slope\n",
        "        \"\"\"\n",
        "        dA = self.activation.backward(dZ)\n",
        "        self.dB = np.mean(dA,axis=0)\n",
        "        self.dW = np.dot(self.X.T,dA)/len(self.X)\n",
        "        dZ = np.dot(dA,self.W.T)\n",
        "\n",
        "        # Update\n",
        "        self = self.optimizer.update(self)\n",
        "\n",
        "        return dZ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1iziegGKSJi"
      },
      "source": [
        "### 【Problem 2】Initialization method\n",
        "Classify the code to do the initialization.\n",
        "\n",
        "As mentioned above, make it possible to pass an instance of the initialization method to the constructor of all binding layers. Please add the necessary code to the following template. The value of the standard deviation (sigma) should be received in the constructor, so that we don't have to pass this value (sigma) in the class of the all-binding layer.\n",
        "\n",
        "The initialization method we have been working with so far will be called the SimpleInitializer class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "lyBybGH2KTJf"
      },
      "outputs": [],
      "source": [
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    Simple initialization with Gaussian distribution\n",
        "    Parameters\n",
        "    ----------\n",
        "    sigma : float\n",
        "      Standard deviation of Gaussian distribution\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Initializing weights\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in subsequent layers\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : weight\n",
        "        \"\"\"\n",
        "        return self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Bias initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in subsequent layers\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : bias\n",
        "        \"\"\"\n",
        "        return np.zeros(n_nodes2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk-XAzbiLExO"
      },
      "source": [
        "### 【Problem 3】Optimization methods\n",
        "Classify the optimization method.\n",
        "\n",
        "The optimization method is also passed as an instance to all coupling layers as in the initialization method. It should be able to be updated like self.optimizer.update(self) when it is backward. Please add the necessary code to the following template.\n",
        "\n",
        "The optimization methods we have covered so far are created as SGD class (Stochastic Gradient Descent)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "fibAaLp6LG1M"
      },
      "outputs": [],
      "source": [
        "class SGD:\n",
        "    \"\"\"\n",
        "    stochastic gradient descent method\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : 学習率\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Updating the weights and biases of a layer\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : An instance of the layer before the update\n",
        "        \"\"\"\n",
        "        layer.W -= self.lr*layer.dW\n",
        "        layer.B -= self.lr*layer.dB\n",
        "\n",
        "        return layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjgSIIDtLNh6"
      },
      "source": [
        "### 【Problem 4】Activation Function\n",
        "Classify the activation function.\n",
        "\n",
        "The backpropagation of the softmax function should be implemented to include the calculation of the cross-entropy error to simplify the calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "JMGFGjJ-LJ9w"
      },
      "outputs": [],
      "source": [
        "class Tanh():\n",
        "    \"\"\"\n",
        "    Activation function : Hyperbolic tangent function\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self,A):\n",
        "        self.A = A\n",
        "        self.Z = np.tanh(self.A)\n",
        "        return self.Z\n",
        "\n",
        "    def backward(self,dZ):\n",
        "        return dZ*(1-self.Z**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "T70m9gaGLREY"
      },
      "outputs": [],
      "source": [
        "class Sigmoid():\n",
        "    \"\"\"\n",
        "    Activation function : Sigmoid function\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self,A):\n",
        "        self.A = A\n",
        "        self.Z = 1/(1+np.exp(-self.A))\n",
        "\n",
        "        return self.Z\n",
        "\n",
        "    def backward(self,dZ):\n",
        "        return dZ*(1-self.Z)*self.Z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "1PzophZqLSiG"
      },
      "outputs": [],
      "source": [
        "class Softmax():\n",
        "    \"\"\"\n",
        "    Activation Function : Softmax Function\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self,A):\n",
        "\n",
        "        return np.exp(A-np.max(A))/np.sum(np.exp(A-np.max(A)),axis=1,keepdims=True)\n",
        "\n",
        "    def backward(self,dZ):\n",
        "        return dZ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWsGicwaLZWj"
      },
      "source": [
        "### 【Problem 5】Creating the ReLU class\n",
        "Implement the (current) commonly used activation function, ReLU (Rectified Linear Unit), as a ReLU class.\n",
        "\n",
        "ReLU is the following formula.  \n",
        "$f(x) = ReLU(x) = \\begin{cases}\n",
        "x  & \\text{if $x>0$,}\\\\\n",
        "0 & \\text{if $x\\leqq0$.}\n",
        "\\end{cases}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "y3Utx5dsLZF4"
      },
      "outputs": [],
      "source": [
        "class ReLU():\n",
        "    \"\"\"\n",
        "    Activation function : ReLU function\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self,A):\n",
        "        self.A = A\n",
        "        return np.maximum(self.A,0)\n",
        "\n",
        "    def backward(self,dZ):\n",
        "\n",
        "        return np.where(self.A>0,dZ,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egMzjCPuLd5m",
        "outputId": "61d1155f-23df-49f9-c5ef-0f3f92c93a94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  0 -1  9  0]\n"
          ]
        }
      ],
      "source": [
        "a = np.array([-1,0,1,9,-1])\n",
        "b = np.array([1,0,-1,9,-1])\n",
        "\n",
        "print(np.where(a<0,0,b))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2tZtWoFLhkc"
      },
      "source": [
        "### 【Problem 6】Initial value of weight\n",
        "So far, we have simply used a Gaussian distribution for the initial values of weights and bias, and treated the standard deviation as a hyperparameter. However, it is known what values should be used. For sigmoid and hyperbolic tangent functions, Xavier's initial value (or Glorot's initial value) is used, and for ReLU, He's initial value is used.\n",
        "\n",
        "Create the XavierInitializer class and the HeInitializer class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "15P4-MjqLjPU"
      },
      "outputs": [],
      "source": [
        "class XavierInitializer():\n",
        "    \"\"\"\n",
        "    Initializing weights with Xavier\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Initializing weights\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in subsequent layers\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : weight\n",
        "        \"\"\"\n",
        "        return np.random.randn(n_nodes1, n_nodes2)/np.sqrt(n_nodes1)\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Bias initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in subsequent layers\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : bias\n",
        "        \"\"\"\n",
        "        return np.zeros(n_nodes2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "SScAM1luLoCq"
      },
      "outputs": [],
      "source": [
        "class HeInitializer():\n",
        "    \"\"\"\n",
        "    Initialization of weights by He\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        Initializing weights\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          Number of nodes in the previous layer\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in subsequent layers\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        W : weight\n",
        "        \"\"\"\n",
        "        return np.random.randn(n_nodes1, n_nodes2)*np.sqrt(2/n_nodes1)\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        Bias initialization\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          Number of nodes in subsequent layers\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        B : bias\n",
        "        \"\"\"\n",
        "        return np.zeros(n_nodes2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBDYYEeoLqZn"
      },
      "source": [
        "### 【Problem 7】Optimization Method\n",
        "The most common method is to change the learning rate during the learning process.  \n",
        "Create a class for AdaGrad, which is the basic method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "Be0wwRAmLru-"
      },
      "outputs": [],
      "source": [
        "class AdaGrad:\n",
        "    \"\"\"\n",
        "    stochastic gradient descent method\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : learning rate\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "        self.hW = None\n",
        "        self.hB = None\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        Updating the weights and biases of a layer\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : An instance of the layer before the update\n",
        "        \"\"\"\n",
        "\n",
        "        if self.hW is None:\n",
        "            self.hW = np.zeros_like(layer.W)  # Initialize hW with the same shape as layer.W\n",
        "        if self.hB is None:\n",
        "            self.hB = np.zeros_like(layer.B)  # Initialize hB with the same shape as layer.B\n",
        "\n",
        "        # Fix: Use element-wise multiplication instead of matrix multiplication\n",
        "        self.hW += layer.dW * layer.dW  # Changed to element-wise multiplication\n",
        "        self.hB += layer.dB * layer.dB  # Changed to element-wise multiplication\n",
        "\n",
        "        layer.W -= self.lr * layer.dW / (np.sqrt(self.hW) + 1e-7)\n",
        "        layer.B -= self.lr * layer.dB / (np.sqrt(self.hB) + 1e-7)\n",
        "\n",
        "        return layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVBL7hRiLunn"
      },
      "source": [
        "### 【Problem 8】Completion of the class\n",
        "Complete the SearchDeepNeuralNetworkClassifier class, which can be trained and estimated in any configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFSYebt2LwMt",
        "outputId": "4f362c47-0313-46b3-9914-131867dc57c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FC Layer: Weights shape - (784, 400), Bias shape - (400,)\n",
            "FC Layer: Weights shape - (400, 200), Bias shape - (200,)\n",
            "FC Layer: Weights shape - (200, 10), Bias shape - (10,)\n"
          ]
        }
      ],
      "source": [
        "clf = ScratchDeepNeuralNetworkClassifier(n_epoch=5, n_features=784,\n",
        "                                         n_nodes1=400, n_nodes2=200, n_output=10,\n",
        "                                         sigma=0.01, n_batch=100,\n",
        "                                         lr = 0.01, verbose = False)\n",
        "\n",
        "clf.fit(X_train,y_train.toarray())\n",
        "y_pred = clf.predict(X_valid)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming y_valid is one-hot encoded, convert it to class labels\n",
        "y_valid_labels = np.argmax(y_valid.toarray(), axis=1)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_valid_labels, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCWK61PkPOoC",
        "outputId": "2b3b1bf0-33de-4e8b-ce33-25d4a3d016c0"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9660833333333333\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7KPefXJmGe223k/jpIMi5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}